{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model Libraries\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "from keras import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Embedding, Reshape, Concatenate\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data\n",
    "\n",
    "- global commodity trade statistics data from kaggle(https://www.kaggle.com/datasets/unitednations/global-commodity-trade-statistics)\n",
    "- longitudinal dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_data = pd.read_csv(\"../gct_dataset/commodity_trade_statistics_data.csv\")\n",
    "gc_data.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data - Only Australia\n",
    "\n",
    "- lmmnn example 참조\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coun_filter = [\"Australia\"]\n",
    "au_data = gc_data[gc_data[\"country_or_area\"].isin(coun_filter)]\n",
    "au_data.reset_index(inplace=True, drop=True)\n",
    "au_data.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_filter = [\"Activated carbon\", \"Alarm clocks, non-electric\", \"Alcoholic liqueurs nes\",\"Almonds in shell fresh or dried\",\"Aluminous cement\"]\n",
    "au_import = au_data[au_data[\"commodity\"].isin(comm_filter)]\n",
    "au_import.reset_index(inplace=True,drop=True)\n",
    "au_import.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Extra Dataset\n",
    "\n",
    "- lmmnn 저자의 전처리 방법 구현\n",
    "- 2가지 데이터(wheat, population)는 미적용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_data = pd.read_csv(\"../gct_dataset/child-mortality-around-the-world.csv\")\n",
    "co_data = pd.read_csv(\"../gct_dataset/co-emissions-per-capita.csv\")\n",
    "dc_data = pd.read_csv(\"../gct_dataset/deaths-conflict-terrorism-per-100000.csv\")\n",
    "tm_data = pd.read_csv(\"../gct_dataset/hadcrut-surface-temperature-anomaly.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_data = tm_data[tm_data[\"Entity\"].isin(coun_filter)]\n",
    "tm_data = tm_data[(tm_data[\"Year\"] >= 1988) & (tm_data[\"Year\"] <= 2016)]\n",
    "tm_data = tm_data.rename(columns={'Entity':'country_or_area',\n",
    "                                  'Year':'year',\n",
    "                                  'Surface temperature anomaly': 'temp_anomaly'})\n",
    "tm_data.drop(\"Code\",axis=1,inplace=True)\n",
    "tm_data.reset_index(inplace=True,drop=True)\n",
    "tm_data.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_data = cm_data[cm_data[\"Entity\"].isin(coun_filter)]\n",
    "cm_data = cm_data[(cm_data[\"Year\"] >= 1988) & (cm_data[\"Year\"] <= 2016)]\n",
    "cm_data = cm_data.rename(columns={'Entity':'country_or_area',\n",
    "                                  'Year':'year',\n",
    "                                  'Child mortality rate - Sex: all - Age: 0-4 - Variant: estimates': 'child_mortality'})\n",
    "cm_data.drop(\"Code\",axis=1,inplace=True)\n",
    "cm_data.reset_index(inplace=True,drop=True)\n",
    "cm_data.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_data = co_data[co_data[\"Entity\"].isin(coun_filter)]\n",
    "co_data = co_data[(co_data[\"Year\"] >= 1988) & (co_data[\"Year\"] <= 2016)]\n",
    "co_data = co_data.rename(columns={'Entity':'country_or_area',\n",
    "                                  'Year':'year',\n",
    "                                  'Annual CO₂ emissions (per capita)': 'co2_emission'})\n",
    "co_data.drop(\"Code\",axis=1,inplace=True)\n",
    "co_data.reset_index(inplace=True,drop=True)\n",
    "co_data.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_data = dc_data[dc_data[\"Entity\"].isin(coun_filter)]\n",
    "dc_data = dc_data[(dc_data[\"Year\"] >= 1988) & (dc_data[\"Year\"] <= 2016)]\n",
    "dc_data = dc_data.rename(columns={'Entity':'country_or_area',\n",
    "                                  'Year':'year',\n",
    "                                  'Deaths - Conflict and terrorism - Sex: Both - Age: All Ages (Rate)': 'death_conflict'})\n",
    "dc_data.drop(\"Code\",axis=1,inplace=True)\n",
    "dc_data.reset_index(inplace=True,drop=True)\n",
    "dc_data.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_import = pd.merge(au_import, tm_data, on=['country_or_area', 'year'], how='left')\n",
    "au_import = pd.merge(au_import, cm_data, on=['country_or_area', 'year'], how='left')\n",
    "au_import = pd.merge(au_import, co_data, on=['country_or_area', 'year'], how='left')\n",
    "au_import = pd.merge(au_import, dc_data, on=['country_or_area', 'year'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_import.fillna(0, inplace=True)\n",
    "au_import.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_df = (\n",
    "    au_import.drop_duplicates(subset='comm_code')\n",
    "    .assign(commodity_id=lambda x: x.groupby('comm_code').ngroup())\n",
    ")\n",
    "distinct_df = distinct_df[[\"comm_code\",\"commodity_id\"]]\n",
    "\n",
    "# inner join 수행\n",
    "au_import = au_import.merge(distinct_df, on='comm_code')\n",
    "\n",
    "# t 계산\n",
    "au_import['t'] = (au_import['year'] - au_import['year'].min()) / (au_import['year'].max() - au_import['year'].min())\n",
    "\n",
    "au_import.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['commodity', 'year', 'comm_code']\n",
    "au_import.drop(cols_to_drop, axis=1, inplace=True)\n",
    "print(au_import.shape)\n",
    "au_import.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Label Encoding\n",
    "nunique = au_import.nunique()\n",
    "types = au_import.dtypes\n",
    "\n",
    "categorical_columns = []\n",
    "categorical_dims =  {}\n",
    "for col in au_import.columns:\n",
    "    print(col, au_import[col].nunique())\n",
    "    if types[col] == 'object' or nunique[col] < 10:\n",
    "        l_enc = LabelEncoder()\n",
    "        au_import[col] = l_enc.fit_transform(au_import[col].values)\n",
    "        categorical_columns.append(col)\n",
    "        categorical_dims[col] = len(l_enc.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_import.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_import['trade_usd'] = np.log(au_import['trade_usd'])\n",
    "au_import['trade_usd'].plot(kind='hist', bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_import.rename(columns={'commodity_id': 'z0', 'trade_usd': 'y'}, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define MeNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Model (Γ(Xi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menet에 사용하고자하는 DNN 모형 정의\n",
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=32, activation='relu', input_dim=12))\n",
    "    model.add(Dense(units=16, activation='relu'))\n",
    "    model.add(Dense(units=1, activation='linear'))\n",
    "    adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay= 0.0001, amsgrad=False)\n",
    "    model.compile(loss='mse', optimizer=adam)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featue Map 추출 하는 함수 정의\n",
    "# DNN 모형의 출력 전 단계의 layer를 추출하여 X의 Feature map을 생성\n",
    "def get_feature_map(model, X):\n",
    "    last_layer = Model(inputs = model.input, outputs = model.layers[-2].output)\n",
    "    return last_layer.predict(X, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Log Likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paper(MeNets)\n",
    "\n",
    "$$ F(g,β,u|X,y)= Σ*{i=1}^{N}[(y_i −Γ(X_i)β−Γ(X_i)u_i)^T Σ*{\\epsilon*i=1}^{-1} (y_i −Γ(X_i)β−Γ(X_i)u_i)+u_i^TΣ*{u}^{-1}u_i +log|Σ\\epsilon_i|+log|Σu|]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paper(LMMNN)\n",
    "$$ NLL(f,g,{\\theta}|y)=1/2(y−{f(X)})^{′}{V(g,{\\theta})^{−1}}(y−{f(X)})+1/2log|V(g,{\\theta})|+n/2log2{\\pi} $$\n",
    "$$ V{(g,{\\theta})} = g(Z)D({\\psi})g(Z)^{′} + {\\sigma}^{2}_{e}I_{n} $$\n",
    "$$ yˆ_{te} = {fˆ}(X_{te} + {gˆ}(Z_{te}){bˆ}) $$\n",
    "$$ {bˆ} = D({\\psi}ˆ){gˆ}(Z_{tr})^{′}V({gˆ},{{\\theta}ˆ})^{-1}(y_{tr} - {fˆ}(X_{tr})) $$\n",
    "$$ V({\\theta}) = ZD(\\psi)Z^{′} + {\\sigma}^{2}_{e}I_{n} = {\\Sigma}^{K-1}_{l=0}{\\Sigma}^{K-1}_{m=0}Z_{l}D_{l,m}Z^{′}_{m} + {\\sigma}^{2}_{e}I_{n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code\n",
    "$$ nll_i = (y_i - f̂_i - Z_i * b̂_i)^T * inv(R̂_i) * (y_i - f̂_i - Z_i * b̂_i) + b̂_i^T * inv(D̂) * b̂_i + log(det(D̂)) + log(det(R̂_i)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단일 클러스터(또는 관측값)에 대한 Negative Log Likelihood 계산\n",
    "def nll_i(y_i, f_hat_i, Z_i, b_hat_i, R_hat_i, D_hat):\n",
    "    # y_i: 클러스터에서의 관측 값\n",
    "    # f_hat_i: 클러스터에서의 예측 값\n",
    "    # Z_i: 클러스터의 feature map\n",
    "    # b_hat_i: estimated random effects for the cluster\n",
    "    # R_hat_i: estimated residual error covariance matrix for the cluster\n",
    "    # D_hat: estimated random effects covariance matrix\n",
    "    return np.transpose(y_i - f_hat_i - Z_i @ b_hat_i) @ np.linalg.inv(R_hat_i) @ (y_i - f_hat_i - Z_i @ b_hat_i) + \\\n",
    "                        b_hat_i @ np.linalg.inv(D_hat) @ b_hat_i + np.log(np.linalg.det(D_hat)) + np.log(np.linalg.det(R_hat_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터셋에서 Negative Log Likelihood 계산 함수\n",
    "def compute_nll(model, X, y, b_hat, D_hat, sig2e_est, maps2ind, n_clusters, cnt_clusters):\n",
    "    # model : Mixed Effect Neural Network의 DNN 모형\n",
    "    # X : 입력 변수\n",
    "    # y : 타겟 변수\n",
    "    # b_hat : 모든 클러스터에 대한 추정된 random effects\n",
    "    # D_hat : 추정된 random effects 공분산 행렬\n",
    "    # sig2e_est : 추정된 잔차 오차 분산\n",
    "    # maps2ind : 클러스터 인덱스에서 데이터 인덱스로의 매핑 정보\n",
    "    # n_clusters : 클러스터의 개수\n",
    "    # cnt_clusters : 클러스터별 데이터 포인트의 수\n",
    "    f_hat = model.predict(X, verbose=0).reshape(X.shape[0]) # 모든 데이터 포인트에 대한 fixed effects 예측\n",
    "    Z = get_feature_map(model, X) # 모든 데이터에 대하여 Feature map 계산\n",
    "    nll = 0 # Negative log likelihood 초기화\n",
    "    for cluster_id in range(n_clusters): # 각 클러스터에 대하여 반복수행 구문\n",
    "        indices_i = maps2ind[cluster_id] # 현재 클러스터의 데이터 포인트 인덱스 호출\n",
    "        n_i = cnt_clusters[cluster_id] # 현재 클러스터의 데이터 포인트 개수 호출\n",
    "        y_i = y[indices_i] # 현재 클러스터에 대한 타겟변수 호출\n",
    "        Z_i = Z[indices_i, :] # 현재 클러스터에 대한 feature map 호출\n",
    "        I_i = np.eye(n_i) # n_i 크기의 항등행렬\n",
    "        f_hat_i = f_hat[indices_i] # 현재 클러스터에 대한 예측값 호출\n",
    "        R_hat_i = sig2e_est * I_i # 현재 클러스터에 대한 추정된 잔차 오차 공분산 행렬 계산\n",
    "        b_hat_i = b_hat[cluster_id, :] # 현재 클러스터에 대한 추정된 random effects 호출\n",
    "        nll = nll + nll_i(y_i, f_hat_i, Z_i, b_hat_i, R_hat_i, D_hat) # 현재 클러스터에 대한 Negative Log Likelihood 계산 후 총합 더하기\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모형 훈련 중에 조기 종료 조건 확인 함수\n",
    "def check_stop_model(nll_valid, best_loss, wait_loss, patience):\n",
    "    # nll_valid : 현재의 검증 손실 값(Negative Log likelihood)\n",
    "    # best_loss : 이전 단계까지의 가장 낮은 검증 손실 값\n",
    "    # wait_loss : 검증 손실이 개선되지 않은 연속적인 에포크 수\n",
    "    # patience : 모형의 검증 손실이 개선되지 않아도 허용하는 최대 에포크 수\n",
    "    # 반환\n",
    "    ## best_loss : 현재 검증 손실이 이전의 최고 손실보다 낮다면 현재의 검증 손실로 업데이트하고, 그렇지 않다면 이전의 최고 손실을 그대로 유지\n",
    "    ## wait_loss : 현재 검증 손실이 이전의 최고 손실보다 낮다면 0으로 초기화하고, 그렇지 않다면 wait_loss를 1 증가\n",
    "    # stop_model : 검증 손실이 개선되지 않은 에포크 수가 patience를 초과하면 True가 되어 모형 훈련을 조기에 종료\n",
    "    stop_model = False\n",
    "    if nll_valid < best_loss:\n",
    "        best_loss = nll_valid\n",
    "        wait_loss = 0\n",
    "    else:\n",
    "        wait_loss += 1\n",
    "        if wait_loss >= patience:\n",
    "            stop_model = True\n",
    "    return best_loss, wait_loss, stop_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize the neural network Γ(0) and βˆ(0)\n",
    "2. Initialize  uˆ_i(0) = 0_q+1, σˆ2_(0) = 1, and Σˆ_u(0) = I\n",
    "3. E-Step : update the fixed part response y^fixed_i(k)\n",
    "\n",
    "    $$ by y^{fixed}_{i(k)} = y_{i} - {\\Gamma}_{i(k-1)}uˆ_{i(k-1)} $$\n",
    "    $$ where {\\Gamma}_{i(k-1)} = {\\Gamma}_{(k-1)}(X_{i}) $$\n",
    "4. Obtain Γ_(k) and β_(k) by training neural networks with data by solving the minimization problem\n",
    "    $$ ({\\Gamma}(k), {\\beta}(k)) = argmin_{({\\Gamma}, {\\beta})} Σ^{n}_{i=1} || y^{fixed}_{i(k)} − {\\Gamma}(X_{i}){\\beta}||^{2} $$\n",
    "5. Update random effects uˆ_k and residuals εˆ_i(k) by\n",
    "    $$ u^{ˆ}_{i(k)} ={\\Sigma}^{ˆ}_{u(k-1)} {\\Gamma}^{T}_{i(k)}V^{ˆ}_{i(k-1)}(y_{i}−{\\Gamma}_{i(k)}{\\beta}^{ˆ}_{(k)}) $$\n",
    "    $$ {\\epsilon}^{ˆ}_{i(k)} = y_{i} - {\\Gamma}_{i(k)}{\\beta}^{ˆ}_{(k)} - {\\Gamma}_{i(k)}u^{ˆ}_{(k)} $$\n",
    "    where $$ V_{i(k-1)} = {\\Sigma}^{ˆ}_{u(k-1)}{\\Gamma}^{T}_{i(k)} + {\\sigma}^{ˆ}_{(k-1)}I_{n} $$\n",
    "    which follow from $$ {E[u^{ˆ}_{i}|y_{i}]} = {\\Sigma}^{ˆ}_{u}{\\Gamma}^{T}_{i}{V^{ˆ}_{i}(y_{i} - {\\Gamma}_{i}{\\beta}^{ˆ})} $$\n",
    "    $$ {E[{\\epsilon}^{ˆ}_{i}|y_{i}]} = {\\sigma}^{2}V^{ˆ-1}_{i}(y_{i} - {\\Gamma}_{i}{\\beta}^{ˆ}) $$\n",
    "    $$ = y_{i} - {\\Gamma}_{i}{\\beta}^{ˆ} - {\\Gamma}_{i}{E[u^{ˆ}_{i}|y_{i}]} $$\n",
    "6. M-Step : update σˆ^2_(k) and Σˆ_u(k) by\n",
    "    $$ {\\sigma}^{ˆ2}_{(k)} = {1/n}{\\Sigma}^{N}_{i=1}(||{\\epsilon}^{ˆ}_{i(k)}||^{2} + {\\sigma}^{ˆ2}_{(k-1)}(n_{i} - {\\sigma}^{ˆ2}_{(k-1)}tr(V^{ˆ}_{i(k-1)}))), $$\n",
    "    $$ {\\Sigma}^{ˆ}_{u(k)} = {1/N}{\\Sigma}^{N}_{i=1}(u^{ˆ}_{i(k)}u^{ˆT}_{i(k)} + {\\Sigma}^{ˆ}_{u(k-1)} - {\\Sigma}^{ˆ}_{u(k-1)}{\\Gamma}^{T}_{u(k-1)}V^{ˆ-1}_{i(k-1)}{\\Gamma}_{i(k)}{\\Sigma}^{ˆ}_{u(k-1)}) $$\n",
    "    when n = Σ^N_i=1 n_i, which follow from\n",
    "    $$ {E[{\\sigma}^{ˆ}|y]} = 1/n {\\Sigma}^{N}_{i=1}(||{\\epsilon}^{ˆ}_{i}||^{2} + {\\sigma}^{2}(n_{i} - {\\sigma}^{2}tr(V^{-1}))) $$\n",
    "    $$ {E[{\\Sigma}^{ˆ}|y]} = 1/N{\\Sigma}^{N}_{i=1}(u^{ˆ}_{i}u^{ˆT}_{i} + {\\Sigma}_{u} - {\\Sigma}_{u}{\\Gamma}^{T}_{i}V^{-1}_{i}{\\Gamma}_{i}{\\Sigma}_u) $$\n",
    "    where $$ {\\sigma}^{ˆ2} = 1/n{\\Sigma}^{N}_{i=1}||{\\epsilon}_{i}||^{2} $$\n",
    "    and $$ {\\Sigma}^{ˆ}_{u} = 1/N{\\Sigma}^{N}_{i=1}u_{i}u^{T}_{i} $$\n",
    "    and MLEs for σ^2 and Σ_u, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def menet_fit(model, X, y, clusters, n_clusters, batch_size, epochs, patience, verbose=False):\n",
    "    # 데이터 세트 분할\n",
    "    X_train, X_valid, y_train, y_valid, clusters_train, clusters_valid = train_test_split(X, y, clusters, test_size=0.1, random_state=0)\n",
    "\n",
    "    # 각 클러스터에 속한 인덱스를 리스트로 만듦\n",
    "    maps2ind_train = [list(np.where(clusters_train == i)[0]) for i in range(n_clusters)]\n",
    "    maps2ind_valid = [list(np.where(clusters_valid == i)[0]) for i in range(n_clusters)]\n",
    "    \n",
    "    # 각 클러스터에 속한 샘플 수를 카운트\n",
    "    cnt_clusters_train = Counter(clusters_train)\n",
    "    cnt_clusters_valid = Counter(clusters_valid)\n",
    "    \n",
    "    # 모델읠 Feature map을 획득\n",
    "    Z = get_feature_map(model, X_train)\n",
    "    \n",
    "    d = Z.shape[1] # Feature map의 차원 수\n",
    "    b_hat = np.zeros((n_clusters, d)) # b_hat 초기화\n",
    "    D_hat = np.eye(d) # D_hat 초기화(단위 행렬)\n",
    "    D_hat_list = []\n",
    "    sig2e_est = 1.0 # 초기의 sig2e 추정값\n",
    "    \n",
    "    # 각 데이터 세트의 Negative Log Likelihood(NLL)을 기록하는 dictionary\n",
    "    nll_history = {'train': [], 'valid': []}\n",
    "    best_loss = np.inf # 최고의 손실값을 기록하는 변수, 초기값은 무한대\n",
    "    wait_loss = 0 # 얼마나 오래 기다려야 하는지 판단하는 변수\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        y_star = np.zeros(y_train.shape) # y_star 초기화\n",
    "        # E - Step\n",
    "        # 각 클러스터에 대하여\n",
    "        for cluster_id in range(n_clusters):\n",
    "            indices_i = maps2ind_train[cluster_id] # 해당 클러스터에 속하는 샘플의 인덱스\n",
    "            b_hat_i = b_hat[cluster_id] # 해당 클러스터의 b_hat\n",
    "            y_star_i = y_train[indices_i] - Z[indices_i] @ b_hat_i # y_star 계산\n",
    "            y_star[indices_i] = y_star_i # y_star 갱신\n",
    "            \n",
    "        # 모델을 y_star 에 대하여 학습\n",
    "        model.fit(X_train, y_star, batch_size = batch_size, epochs=1, verbose=0)\n",
    "        \n",
    "        Z = get_feature_map(model, X_train) # 모델의 feature map 갱신\n",
    "        \n",
    "        f_hat = model.predict(X_train, verbose=0).reshape(X_train.shape[0]) # 모델의 예측값\n",
    "        sig2e_est_sum = 0 # sig2e 추정값의 합을 초기화\n",
    "        D_hat_sum = 0 # D_hat의 합을 초기화\n",
    "        \n",
    "        # M - Step\n",
    "        # 각 클러스터에 대하여\n",
    "        for cluster_id in range(n_clusters):\n",
    "            indices_i = maps2ind_train[cluster_id] # 해당 클러스터에 속하는 샘플의 인덱스\n",
    "            n_i = cnt_clusters_train[cluster_id] # 해당 클러스터에 속하는 샘플의 수\n",
    "            f_hat_i = f_hat[indices_i] # 해당 클러스터에 속하는 샘플의 예측값\n",
    "            y_i = y_train[indices_i] # 해당 클러스터에 속하는 샘플의 실제값\n",
    "            Z_i = Z[indices_i, :] # 해당 클러스터에 속하는 샘플의 feature map\n",
    "            \n",
    "            # V_hat 계산\n",
    "            V_hat_i = Z_i @ D_hat @ np.transpose(Z_i) + sig2e_est * np.eye(n_i)\n",
    "            V_hat_inv_i =  np.linalg.inv(V_hat_i) # V_hat의 역행렬\n",
    "            b_hat_i = D_hat @ np.transpose(Z_i) @ V_hat_inv_i @ (y_i - f_hat_i)# b_hat 갱신\n",
    "            eps_hat_i = y_i - f_hat_i - Z_i @ b_hat_i # 잔차 계산\n",
    "            b_hat[cluster_id, :] = b_hat_i.squeeze() # b_hat 갱신\n",
    "            # sig2e 추정값의 합 갱신\n",
    "            sig2e_est_sum = sig2e_est_sum + np.transpose(eps_hat_i) @ eps_hat_i + sig2e_est * (n_i - sig2e_est * np.trace(V_hat_inv_i))\n",
    "            # D_hat의 합 갱신\n",
    "            D_hat_sum = D_hat_sum + b_hat_i @ np.transpose(b_hat_i) + (D_hat - D_hat @ np.transpose(Z_i) @ V_hat_inv_i @ Z_i @ D_hat)\n",
    "            \n",
    "        sig2e_est = sig2e_est_sum / X_train.shape[0] # sig2e 추정값 갱신\n",
    "        D_hat = D_hat_sum / n_clusters # D_hat 계산\n",
    "        D_hat_list.append(D_hat)\n",
    "        \n",
    "        # NLL 계산\n",
    "        # nll_train = compute_nll(model, X_train, y_train, b_hat, D_hat, sig2e_est, maps2ind_train, n_clusters, cnt_clusters_train)\n",
    "        nll_valid = compute_nll(model, X_valid, y_valid, b_hat, D_hat, sig2e_est, maps2ind_valid, n_clusters, cnt_clusters_valid)\n",
    "        # nll_history['train'].append(nll_train)\n",
    "        nll_history['valid'].append(nll_valid)\n",
    "        best_loss, wait_loss, stop_model = check_stop_model(nll_valid, best_loss, wait_loss, patience)\n",
    "        \n",
    "        # 현재 상태 출력\n",
    "        if verbose:\n",
    "            print(f'epoch: {epoch}, val_loss: {nll_valid:.2f}, sig2e_est: {sig2e_est:.2f}')\n",
    "        # 모델을 멈출 경우, 반복문 종료\n",
    "        if stop_model:\n",
    "            break\n",
    "        \n",
    "    n_epochs = len(nll_history['valid']) # 총 epoch 수\n",
    "    return model, b_hat, sig2e_est, n_epochs, nll_history, D_hat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = int(max([au_import['z0'].max()])+ 1)\n",
    "\n",
    "train, test = train_test_split(au_import, test_size=0.2, random_state=530)\n",
    "\n",
    "X_cols = list(au_import.columns)\n",
    "X_cols.remove('y')\n",
    "y_cols = ['y']\n",
    "\n",
    "X_train = train[X_cols].values\n",
    "y_train = train[y_cols].values.squeeze()\n",
    "clusters_train = train['z0'].values\n",
    "# clusters_train = clusters_train.reshape(clusters_train.shape[0],1)\n",
    "\n",
    "X_test = test[X_cols].values\n",
    "y_test = test[y_cols].values.squeeze()\n",
    "clusters_test = test['z0'].values\n",
    "# clusters_test = clusters_test.reshape(clusters_test.shape[0],1)\n",
    "\n",
    "print(X_train.shape, y_train.shape, clusters_train.shape)\n",
    "print(X_test.shape, y_test.shape, clusters_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "patience = 5\n",
    "verbose=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menet fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "menet_model, b_hat, sig2e_est, n_epochs, nll_history, D_hat_list = menet_fit(model, X_train, y_train, clusters_train, q, batch_size, epochs, patience, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check D_hat_list\n",
    "print(len(D_hat_list))\n",
    "print(np.array(D_hat_list).shape)\n",
    "print(D_hat_list[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_hat_list[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MeNet Predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def menet_predict(model, X, clusters, n_clusters, b_hat):\n",
    "    y_hat = model.predict(X, verbose=0).reshape(X.shape[0])\n",
    "    Z = get_feature_map(model, X)\n",
    "    for cluster_id in range(n_clusters):\n",
    "        indices_i = np.where(clusters == cluster_id)[0]\n",
    "        if len(indices_i) == 0:\n",
    "            continue\n",
    "        b_i = b_hat[cluster_id, :]\n",
    "        Z_i = Z[indices_i, :]\n",
    "        y_hat[indices_i] = y_hat[indices_i] + Z_i @ b_i\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = menet_predict(model, X_test, clusters_test, q, b_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.mean((y_pred - y_test)**2)\n",
    "rmse = np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
